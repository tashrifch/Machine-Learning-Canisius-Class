{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c9431f-b877-4cc4-af8c-0323577c44f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Viewing Embedding using TensorBoard\n",
    "\n",
    "We saw Embedding in the discussion of text classification,  this Notebook shows how we can view embedding in TensorBoard\n",
    "\n",
    "Make sure you have completed the Python notebooks on working with text before attempting to run this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd68fc9f-497a-4a6d-b08e-907cdc2e906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600d4718-b421-4434-acbd-6aadf4eb9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard does not shut down cleanly on it's own.  You have to manually delete a file so that it does not restart\n",
    "# using the previous data set.   Shut it down (on Windows) using this command,   you need to change the user name from hdavi to your\n",
    "# user name\n",
    "\n",
    "dir = \"C:\\\\Users\\\\hdavi\\\\AppData\\\\Local\\\\Temp\\\\.tensorboard-info\"\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0892a4-1b67-4bd2-8dc7-f07d7582e33a",
   "metadata": {},
   "source": [
    "## Delete those logfiles first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06aefd5a-1704-4bd5-9f7e-8c968830624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    \"imdb_reviews/subwords8k\",\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "encoder = info.features[\"text\"].encoder\n",
    "\n",
    "# Shuffle and pad the data.\n",
    "train_batches = train_data.shuffle(1000).padded_batch(\n",
    "    10, padded_shapes=((None,), ())\n",
    ")\n",
    "test_batches = test_data.shuffle(1000).padded_batch(\n",
    "    10, padded_shapes=((None,), ())\n",
    ")\n",
    "train_batch, train_labels = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4291f16-79c5-468e-82be-875aa2fe8643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not sure why the underscores got added.\n",
    "encoder.subwords[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed7cc18-728a-4341-926b-3f736ed27472",
   "metadata": {},
   "source": [
    "## A quick text classifier using embedding in a Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d885090d-93ff-4f08-9a89-b8dd9a3b0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "# Create an embedding layer.\n",
    "embedding_dim = 16\n",
    "embedding = Embedding(encoder.vocab_size, embedding_dim)\n",
    "\n",
    "# Configure the embedding layer as part of a keras model.\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        embedding, # The embedding layer should be the first layer in a model.\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile model.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcde1ec-2954-4c00-b51b-c661413b4350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 10s 4ms/step - loss: 0.5146 - accuracy: 0.6891 - val_loss: 0.4286 - val_accuracy: 0.8450\n"
     ]
    }
   ],
   "source": [
    "# Train model for one epoch.  Increase this if the code runs fast\n",
    "history = model.fit(\n",
    "    train_batches, epochs=1, validation_data=test_batches, validation_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c56105-8912-4dc7-b57d-f119953c6984",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Failed to rename: /logs/imdb-example/embedding.ckpt-1_temp/part-00000-of-00001.data-00000-of-00001 to: /logs/imdb-example/embedding.ckpt-1.data-00000-of-00001 : Access is denied.\r\n; Input/output error [Op:MergeV2Checkpoints]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create a checkpoint from embedding, the filename and key are the\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# name of the tensor.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mCheckpoint(embedding\u001b[38;5;241m=\u001b[39mweights)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Set up config.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m config \u001b[38;5;241m=\u001b[39m projector\u001b[38;5;241m.\u001b[39mProjectorConfig()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:2170\u001b[0m, in \u001b[0;36mCheckpoint.save\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m   2168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2169\u001b[0m   checkpoint_number \u001b[38;5;241m=\u001b[39m assign_op\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m-> 2170\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2171\u001b[0m \u001b[43m                       \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2172\u001b[0m checkpoint_management\u001b[38;5;241m.\u001b[39mupdate_checkpoint_state_internal(\n\u001b[0;32m   2173\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(file_prefix),\n\u001b[0;32m   2174\u001b[0m     model_checkpoint_path\u001b[38;5;241m=\u001b[39mfile_path,\n\u001b[0;32m   2175\u001b[0m     all_model_checkpoint_paths\u001b[38;5;241m=\u001b[39m[file_path],\n\u001b[0;32m   2176\u001b[0m     save_relative_paths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:2071\u001b[0m, in \u001b[0;36mCheckpoint.write\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m   2069\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2070\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[1;32m-> 2071\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2073\u001b[0m _checkpoint_write_durations\u001b[38;5;241m.\u001b[39mget_cell(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m   2074\u001b[0m     _get_duration_microseconds(start_time, end_time))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1262\u001b[0m, in \u001b[0;36mTrackableSaver.save\u001b[1;34m(self, file_prefix, checkpoint_number, session, options)\u001b[0m\n\u001b[0;32m   1259\u001b[0m   object_graph_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m file_io\u001b[38;5;241m.\u001b[39mrecursive_create_dir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(file_prefix))\n\u001b[1;32m-> 1262\u001b[0m save_path, new_feed_additions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_cached_when_graph_building\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_graph_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_feed_additions:\n\u001b[0;32m   1265\u001b[0m   feed_dict\u001b[38;5;241m.\u001b[39mupdate(new_feed_additions)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:1208\u001b[0m, in \u001b[0;36mTrackableSaver._save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor, options)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_save_object_graph \u001b[38;5;241m!=\u001b[39m graph_proto\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;66;03m# When executing eagerly, we need to re-create SaveableObjects each time\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;66;03m# save() is called so they pick up new Tensors passed to their\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;66;03m# constructors. That means the Saver needs to be copied with a new\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;66;03m# var_list.\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function()):\n\u001b[0;32m   1207\u001b[0m   saver \u001b[38;5;241m=\u001b[39m functional_saver\u001b[38;5;241m.\u001b[39mMultiDeviceSaver(named_saveable_objects)\n\u001b[1;32m-> 1208\u001b[0m   save_op \u001b[38;5;241m=\u001b[39m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/cpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([save_op]):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:300\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save\u001b[1;34m(self, file_prefix, options)\u001b[0m\n\u001b[0;32m    298\u001b[0m   tf_function_save()\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msave_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py:286\u001b[0m, in \u001b[0;36mMultiDeviceSaver.save.<locals>.save_fn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m merge_device \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    280\u001b[0m     options\u001b[38;5;241m.\u001b[39mexperimental_io_device \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     saveable_object_util\u001b[38;5;241m.\u001b[39mset_cpu0(last_device))\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mdevice(merge_device):\n\u001b[0;32m    283\u001b[0m   \u001b[38;5;66;03m# V2 format write path consists of a metadata merge step.  Once\u001b[39;00m\n\u001b[0;32m    284\u001b[0m   \u001b[38;5;66;03m# merged, attempts to delete the temporary directory,\u001b[39;00m\n\u001b[0;32m    285\u001b[0m   \u001b[38;5;66;03m# \"<user-fed prefix>_temp\".\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_io_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_v2_checkpoints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m      \u001b[49m\u001b[43msharded_prefixes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_old_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:501\u001b[0m, in \u001b[0;36mmerge_v2_checkpoints\u001b[1;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name)\u001b[0m\n\u001b[0;32m    499\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 501\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_v2_checkpoints_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheckpoint_prefixes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdelete_old_dirs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_old_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py:526\u001b[0m, in \u001b[0;36mmerge_v2_checkpoints_eager_fallback\u001b[1;34m(checkpoint_prefixes, destination_prefix, delete_old_dirs, name, ctx)\u001b[0m\n\u001b[0;32m    524\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [checkpoint_prefixes, destination_prefix]\n\u001b[0;32m    525\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete_old_dirs\u001b[39m\u001b[38;5;124m\"\u001b[39m, delete_old_dirs)\n\u001b[1;32m--> 526\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMergeV2Checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _result\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\dl3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to rename: /logs/imdb-example/embedding.ckpt-1_temp/part-00000-of-00001.data-00000-of-00001 to: /logs/imdb-example/embedding.ckpt-1.data-00000-of-00001 : Access is denied.\r\n; Input/output error [Op:MergeV2Checkpoints]"
     ]
    }
   ],
   "source": [
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "# on a windows machine this will be at c:/logs/imdb-example\n",
    "\n",
    "log_dir='/logs/imdb-example/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Save Labels separately on a line-by-line manner.\n",
    "# I added code here to convert the string encoding to utf-8 and remove the underscore\n",
    "\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "  for subwords in encoder.subwords:\n",
    "    stemp=str(subwords.encode('utf-8'))\n",
    "    stemp=stemp[1:].strip(\"'\")\n",
    "    stemp=stemp.strip(\"_\")\n",
    "    f.write(\"{}\\n\".format(stemp))\n",
    "  # Fill in the rest of the labels with \"unknown\".\n",
    " # for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "#    f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "\n",
    "# Save the weights we want to analyze as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, here\n",
    "# we will remove this value.\n",
    "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "# Create a checkpoint from embedding, the filename and key are the\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896a99d8-83b5-43d6-8044-5673608803a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f79ca0f-eff4-42e4-a281-34cb71547e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16096), started 0:00:04 ago. (Use '!kill 16096' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13d4a4b57f121a09\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13d4a4b57f121a09\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if tensorboard does not see the logs the first time, try restarting it by running this cell again\n",
    "\n",
    "# check to see if the logdir setting the window below matches the logdir setting in the call of TensorBoard.  If it does not, exit Jupyter, shut down the \n",
    "# Jupyter server, delete all the log files and restart Jupyter,  run the notebook again.\n",
    "\n",
    "Enable the three-D labels mode to see specific words\n",
    "\n",
    "%tensorboard --logdir /logs/imdb-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f26728ac-95b1-44cb-88b3-b32f8eee7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard does not shut down cleanly on it's own.  You have to manually delete a file so that it does not restart\n",
    "# using the previous data set.   Shut it down (on Windows) using this command,   you need to change the user name from hdavi to your\n",
    "# user name\n",
    "\n",
    "dir = \"C:\\\\Users\\\\hdavi\\\\AppData\\\\Local\\\\Temp\\\\.tensorboard-info\"\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c313e462-4f6e-491d-8f7c-00c3af9b32cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-77cda27ab971e205\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-77cda27ab971e205\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if tensorboard does not see the logs the first time, try restarting it by running this cell again\n",
    "\n",
    "# check to see if the logdir setting the window below matches the logdir setting in the call of TensorBoard.  If it does not, exit Jupyter, shut down the \n",
    "# Jupyter server, delete all the log files and restart Jupyter,  run the notebook again.\n",
    "\n",
    "%tensorboard --logdir /logsb/imdb-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94cf6290-c5ba-46f4-910c-1d7410d6a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-03-27T15:18:22.953623-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.7\n",
      "IPython version      : 8.1.1\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 158 Stepping 13, GenuineIntel\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0607056e-18e9-44fc-b9cd-616940c567f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard        : 2.6.0\n",
      "tensorflow         : 2.6.0\n",
      "tensorflow_datasets: 4.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ae827-7674-4f99-a71b-881398841be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
